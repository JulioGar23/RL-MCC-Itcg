{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89627e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-Greedy Action-Value Estimation\n",
    "# Sample Average Method for Estimating Action Values\n",
    "# RL Course - JAGR\n",
    "\n",
    "import random\n",
    "\n",
    "# Bottles\n",
    "bottle_actions = [\"P\", \"Y\", \"B\"]\n",
    "q_values = [0.0, 0.0, 0.0]  \n",
    "counts = [0, 0, 0] \n",
    "acum_rewards = [0, 0, 0] \n",
    "\n",
    "\n",
    "# Real World Simulation (Hidden Probabilities, Hidden Distributions)\n",
    "# Hidden to the agent\n",
    "real_probabilities=[0.25, 0.75, 0.50]\n",
    "\n",
    "for t in range(1, 100): # iterations\n",
    "    selected_index = random.randint(0, 2)   # Random action selection (0, 1, or 2)\n",
    "    action = bottle_actions[selected_index]\n",
    "\n",
    "    if random.random() < real_probabilities[selected_index]:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    counts[selected_index] += 1\n",
    "    acum_rewards[selected_index] += reward\n",
    "\n",
    "    q_values[selected_index] = acum_rewards[selected_index] / counts[selected_index]\n",
    "\n",
    "    #print(f\"Iteration {t}: Selected Bottle: {action}, Reward: {reward}, Q-values: {q_values}, Acumulated Rewards: {acum_rewards}, Counts: {counts}\")\n",
    "\n",
    "# Final Q-values after all iterations\n",
    "print(f\"Final Q-values: {q_values}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Q-values over Steps\n",
    "# Plotting version - Same as above but with plotting\n",
    "# JAGR - RL Course\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(12)\n",
    "\n",
    "# Bottles\n",
    "bottle_actions = [\"P\", \"Y\", \"B\"]\n",
    "q_values = [0.0, 0.0, 0.0]  \n",
    "counts = [0, 0, 0] \n",
    "acum_rewards = [0, 0, 0] \n",
    "n_steps = 10000\n",
    "\n",
    "q_values_history = [[], [], []]  # To store Q-values over time\n",
    "\n",
    "# Real World Simulation (Hidden Probabilities, Hidden Distributions)\n",
    "# Hidden to the agent\n",
    "real_probabilities=[0.25, 0.75, 0.50]\n",
    "\n",
    "for t in range(1, n_steps): # iterations\n",
    "    selected_index = random.randint(0, 2)   # Random action selection (0, 1, or 2)\n",
    "    action = bottle_actions[selected_index]\n",
    "\n",
    "    if random.random() < real_probabilities[selected_index]:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    counts[selected_index] += 1\n",
    "    acum_rewards[selected_index] += reward\n",
    "\n",
    "    q_values[selected_index] = acum_rewards[selected_index] / counts[selected_index]\n",
    "    \n",
    "    # Store Q-values for plotting\n",
    "    for i in range(3):\n",
    "        q_values_history[i].append(q_values[i])\n",
    "\n",
    "    #print(f\"Iteration {t}: Selected Bottle: {action}, Reward: {reward}, Q-values: {q_values}, Acumulated Rewards: {acum_rewards}, Counts: {counts}\")\n",
    "\n",
    "# Final Q-values after all iterations\n",
    "print(f\"Final Q-values: {q_values}\")\n",
    "\n",
    "# Plot Q-values over time\n",
    "plt.plot(range(1, n_steps), q_values_history[0], label='Bottle P')\n",
    "plt.plot(range(1, n_steps), q_values_history[1], label='Bottle Y')\n",
    "plt.plot(range(1, n_steps), q_values_history[2], label='Bottle B')\n",
    "plt.xlabel(\"Simulation Steps (t)\")     \n",
    "plt.ylabel(\"Estimated average reward ($Q_t$)\")\n",
    "plt.title(\"Q-values over steps for each bottle\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Percentage of Optimal Action Selection over Steps\n",
    "# JAGR - RL Course\n",
    "\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random.seed(12)\n",
    "\n",
    "# Configuration\n",
    "n_steps = 5000\n",
    "n_runs = 200  \n",
    "real_probabilities = [0.25, 0.75, 0.50]\n",
    "optimal_action = 1  \n",
    "\n",
    "# Matrix to store if the optimal action was chosen (Rows: runs, Columns: steps)\n",
    "# Initilized with zeros, will be set to 1 when the optimal action is chosen\n",
    "optimal_choices_history = np.zeros((n_runs, n_steps))\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Reset for each run\n",
    "    counts = [0, 0, 0]\n",
    "    acum_rewards = [0, 0, 0]\n",
    "    q_values = [0.0, 0.0, 0.0]\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        # Policy: Random action selection (0, 1, or 2)\n",
    "        selected_index = random.randint(0, 2) \n",
    "\n",
    "        # Registred if the optimal action was selected\n",
    "        if selected_index == optimal_action:\n",
    "            optimal_choices_history[run, t] = 1\n",
    "\n",
    "        # Simulation of the environment response (reward)\n",
    "        reward = 1 if random.random() < real_probabilities[selected_index] else 0\n",
    "\n",
    "        # Update (Sample Average)\n",
    "        counts[selected_index] += 1\n",
    "        acum_rewards[selected_index] += reward\n",
    "        q_values[selected_index] = acum_rewards[selected_index] / counts[selected_index]\n",
    "\n",
    "# Computing the percentage of optimal action selection over time\n",
    "# This will give us a curve showing how often the optimal action was chosen at each step, averaged over all runs\n",
    "percent_optimal_action = np.mean(optimal_choices_history, axis=0) * 100\n",
    "\n",
    "# Plotting the percentage of optimal action selection over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(n_steps), percent_optimal_action)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"% Optimal Action\")\n",
    "plt.title(\"Performance: Percentage of Optimal Action Selection (Random Policy)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
