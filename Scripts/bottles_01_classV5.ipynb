{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd765531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-type for the medical bottles experiment\n",
    "# Fifth version by JAGR.\n",
    "# Sample average method to update action-values (Q-values) using ARGMAX action selection and q_values initialized to [1.0] Explotation-only.\n",
    "# Here, the agent always exploits the current knowledge without exploration.\n",
    "# With q_values initialized optimistically, force the agent  to try all bottles at least once. Most of the time the agent exploits the best-known bottle.\n",
    "\n",
    "import random\n",
    "\n",
    "class bottle:\n",
    "    def __init__(self, name, real_prob):\n",
    "        # Constructor: it executes when you create a new bottle\n",
    "        # Here we define the initial states of the bottle. Attributes.\n",
    "        self.name = name\n",
    "        self.real_prob = real_prob\n",
    "\n",
    "        self.q_value = 1.0  # Initial estimate of the value of the bottle\n",
    "        self.count = 0      # Number of times the bottle has been used\n",
    "        self.accum_reward = 0.0  # Total reward obtained from this bottle\n",
    "\n",
    "    def try_bottle(self):\n",
    "        # Simulate trying the bottle\n",
    "        if random.random() < self.real_prob:\n",
    "            return 1  # Reward of 1 if successful\n",
    "        else:\n",
    "            return 0  # Reward of 0 if not successful\n",
    "\n",
    "    def learn(self, reward):\n",
    "        # Update the Q-value based on the received reward\n",
    "        self.count += 1\n",
    "        self.accum_reward += reward\n",
    "        if self.count > 0:\n",
    "            self.q_value = self.accum_reward / self.count  # Update Q-value as average reward\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Bottle {self.name} -> Q: {self.q_value:.4f} (Chosen {self.count} times)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91932843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bottles with different real probabilities\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "bottles = [\n",
    "    bottle(\"P\", 0.25), \n",
    "    bottle(\"Y\", 0.75), \n",
    "    bottle(\"B\", 0.50)\n",
    "    ]\n",
    "\n",
    "tried_bottles_history = []                              # To record history of tried bottles\n",
    "q_values_history = []                                   # To record history of Q-values    \n",
    "accumulated_rewards_history = []                        # To record accumulated rewards\n",
    "\n",
    "n_steps = 10000\n",
    "for t in range(1, n_steps+1):\n",
    "    \n",
    "    selected_bottle = bottles[0]\n",
    "    for b in bottles:\n",
    "        if b.q_value > selected_bottle.q_value:\n",
    "            selected_bottle = b    \n",
    "\n",
    "    reward = selected_bottle.try_bottle()\n",
    "    selected_bottle.learn(reward)\n",
    "    tried_bottles_history.append(selected_bottle.name)\n",
    "    q_values_history.append([i.q_value for i in bottles])\n",
    "    accumulated_rewards_history.append([j.accum_reward for j in bottles])\n",
    "    #print(f\"Iteration {t}: Tried bottle {selected_bottle.name}, Reward: {reward}, Q-value: {selected_bottle.q_value:.4f}, Count: {selected_bottle.count}, Accumulated Reward: {selected_bottle.accum_reward}\")\n",
    "\n",
    "for i in bottles:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42088236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(tried_bottles_history, bins=np.arange(-0.5, 3.5, 1), rwidth=0.8)\n",
    "plt.xticks([0, 1, 2], ['P', 'Y', 'B'])\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('Bottles')\n",
    "plt.ylabel('Number of times tried')\n",
    "plt.title('Histogram of Tried Bottles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f133084",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, n_steps+1), np.array(q_values_history)[:,0], label='Bottle P')\n",
    "plt.plot(range(1, n_steps+1), np.array(q_values_history)[:,1], label='Bottle Y')\n",
    "plt.plot(range(1, n_steps+1), np.array(q_values_history)[:,2], label='Bottle B')\n",
    "plt.grid()\n",
    "plt.xlabel('Simulation Steps')         \n",
    "plt.ylabel('Q-values')\n",
    "plt.title('$Q_t(a)$ estimation over Time - Sample Average Method')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ee807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, n_steps+1), np.array(accumulated_rewards_history)[:,0], label='Bottle P')\n",
    "plt.plot(range(1, n_steps+1), np.array(accumulated_rewards_history)[:,1], label='Bottle Y')\n",
    "plt.plot(range(1, n_steps+1), np.array(accumulated_rewards_history)[:,2], label='Bottle B')\n",
    "plt.grid()\n",
    "plt.xlabel('Simulation Steps')         \n",
    "plt.ylabel('Accumulated Rewards')\n",
    "plt.title('Accumulated Rewards - Sample Average Method')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
